---
title: "Using the Weight Lifting Exercise Dataset to Evaluate Correct Lifting Form"
output: html_document
---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The objective of this project is to train a model to tell a weight lifter if they are doing the exercise correctly or how they are doing it wrong.

The training data being used for this project is available from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and the test data is available from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv. Here the data is loaded into R.

```{r}
setwd("~/datasciencecoursera/Practical Machine Learning/courseProject/")
data <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

The data contains a lot of predictors which are predominantly NA values or empty strings. These predictors can be removed to speed up the building of the model. Other columns being removed are names of the participants, time stamps and the labelling of the measurement windows. To an end user in the everyday environment of the gym these predictors will make no sense.

```{r}
data <- data[,!(data[1,] == "" | is.na(data[1,]))]
data <- data[,-7:-1]
testing <- testing[,!(testing[1,] == "" | is.na(testing[1,]))]
testing <- testing[,-7:-1]
```

The data is then split into a smaller training set and a larger test set. There are two reasons for this:

* Building a random forest model with the full set of training data is very computationally intensive and can take a long time. Good accuracies can be achieved with just a smaller fraction.
* If we do this multiple times it is cross validation which can be used to predict out of sample accuracy. We will build three models and take the average of the accuracy of the models.

```{r cache=TRUE}
library(kernlab); library(caret)
set.seed(453)
inTrain1 <- createDataPartition(y=data$classe, p=0.2, list=FALSE)
training1 <- data[inTrain1,]
testing1 <- data[-inTrain1,]
model1 <- train(classe ~ ., data=training1, method="rf")
set.seed(7823)
inTrain2 <- createDataPartition(y=data$classe, p=0.2, list=FALSE)
training2 <- data[inTrain2,]
testing2 <- data[-inTrain2,]
model2 <- train(classe ~ ., data=training2, method="rf")
set.seed(199)
inTrain3 <- createDataPartition(y=data$classe, p=0.2, list=FALSE)
training3 <- data[inTrain3,]
testing3 <- data[-inTrain3,]
model3 <- train(classe ~ ., data=training3, method="rf")
```

We can then make the predictions and calculate the expected out of sample accuracy:

```{r cache=TRUE}
prediction1 <- predict(model1, testing1)
prediction2 <- predict(model2, testing2)
prediction3 <- predict(model3, testing3)
paste(round(100*mean(confusionMatrix(prediction1, testing1$classe)[3][[1]][1],
     confusionMatrix(prediction2, testing2$classe)[3][[1]][1],
     confusionMatrix(prediction3, testing3$classe)[3][[1]][1]), 2), "%", sep="")
```

The models give the following following predictions on the original (non-cross validation) testing set

```{r}
as.character(predict(model1, testing))
as.character(predict(model2, testing))
as.character(predict(model3, testing))
```

And finally, the test predictions for submission can be created in the correct format like so

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(as.character(predict(model1, testing)))
```